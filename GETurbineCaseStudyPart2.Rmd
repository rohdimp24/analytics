---
title: "GETurbineCaseStudyPart2"
output: html_document
---

read the training data

```{r}
filenameToRead<-"train.csv"
turbineTrainData<-read.csv(filenameToRead)
str(turbineTrainData)

```

Performing some explorations

```{r}
#Find the summary of the values
summary(turbineTrainData)

```

Now drawing the box plts to see the distribution

```{r}
#boxplots
turbines<-names(turbineTrainData)
boxplot(turbineTrainData,names=turbines,main="Box plot comparing all turbines")
abline(a=median(turbineTrainData$X190),b=0,col="red",lty=2)
abline(a=summary(turbineTrainData$X190)[2],b=0,col="blue",lty=2)
abline(a=summary(turbineTrainData$X190)[5],b=0,col="blue",lty=2)


```
The box plot shows that for some of the turbines are producing the data similar to X190 while there are others which are not similar. So they might not have much impact while developing the model

Now checking the correlations

```{r}
cor(turbineTrainData)
```


__Generating models__

Regression Model1
using all the independent variables  

```{r}
model1=lm(X190~.,data=turbineTrainData)
summary(model1)

```


Regression Model2

After removing the not so important variables
```{r}
model2=lm(X190~X180+X186+X188+X189+X191+X193+X200,data=turbineTrainData)
summary(model2)

```

Check for the correlations in this model

```{r}
subDataFrame<-turbineTrainData[c('X180','X186','X188','X189','X190','X191','X193','X200')]
corSubData<-cor(subDataFrame)
corSubData


```

Regression Model3

After removing the correlated varibales

```{r}
model3=lm(X190~X180+X186+X189+X191+X200,data=turbineTrainData)
summary(model3)
```

There is a scope to remove the not so important variable X180

Regression Model4

After removing X180
```{r}
model4=lm(X190~X186+X189+X191+X200,data=turbineTrainData)
summary(model4)
trainSSE4=sum(model4$residuals^2)
trainRMSE4=sqrt(trainSSE4/nrow(turbineTrainData))
trainR2Calc4=1-trainSSE4/trainSST
trainRSqr4=summary(model4)$adj.r.squared
MSE.multipleLinreg=trainSSE4
RMSE.multipleLinReg=trainRMSE4
summary(model4)$sigma

```

So the model4 is the final regression model

__Non Linear models __

__Decision tree__

Creating a Bushy tree
```{r}
library(tree)
tree.train=tree(X190~.,data=turbineTrainData)
plot(tree.train)
text(tree.train,pretty=0)

tree.pred=predict(tree.train)
MSE.bushy=mean((tree.pred-turbineTrainData$X190)^2)
RMSE.bushy=sqrt(MSE.bushy)
RMSE.bushy

```

Determining the prune levels

```{r}
cv.turbine = cv.tree(tree.train)
cv.turbine
plot(cv.turbine$size ,cv.turbine$dev ,type='b', main="Plot between size of the tree and the deviance",xlab = "Size of tree", ylab= "deviance")
```
This gives the tree size for which the deviance will be the smallest. In this case it is at 6 which is also the size of the bushy tree. so no pruning is required

Note that the leaves of the tree are similar to the independent variables in the final regression model 

__Bagging__

```{r}

library(randomForest)
bag.turbine=randomForest(X190 ~.,data=turbineTrainData,mtry=18,importance=TRUE)
bag.predict=predict(bag.turbine)
MSE.bag=mean((bag.predict-turbineTrainData$X190)^2)
RMSE.bag=sqrt(MSE.bag)
RMSE.bag

```

Lets see the importance varaibles 

```{r}

importance(bag.turbine)

```
we find that similar variables as in the case of the regression model have been given the more importance

