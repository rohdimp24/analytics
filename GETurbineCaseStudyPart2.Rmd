---
title: "GETurbineCaseStudy Part2"
output: html_document
---

__Problem__
Now use the data from the entire wind farm to perform exploratory analysis and create a predictive model of turbine 190 power given the power from the rest of the farm.  Does this improve the estimate from above?


__Solution__

read the training data

```{r}
filenameToRead<-"Wind Turbine Power Data_Train_GE Internal.csv"
turbineTrainData<-read.csv(filenameToRead)
str(turbineTrainData)

```

Performing some explorations

```{r}
#Find the summary of the values
summary(turbineTrainData)

```

Now drawing the box plts to see the distribution

```{r}
#boxplots
turbines<-names(turbineTrainData)
boxplot(turbineTrainData,names=turbines,main="Box plot comparing all turbines")
abline(a=median(turbineTrainData$X190),b=0,col="red",lty=2)
abline(a=summary(turbineTrainData$X190)[2],b=0,col="blue",lty=2)
abline(a=summary(turbineTrainData$X190)[5],b=0,col="blue",lty=2)


```
The box plot shows that for some of the turbines are producing the data similar to X190 while there are others which are not similar. So they might not have much impact while developing the model

Now checking the correlations

```{r}
cor(turbineTrainData)
```


__Generating models__

Regression Model1
using all the independent variables  

```{r}
model1=lm(X190~.,data=turbineTrainData)
summary(model1)

```


Regression Model2

After removing the not so important variables
```{r}
model2=lm(X190~X180+X186+X188+X189+X191+X193+X200,data=turbineTrainData)
summary(model2)

```

Check for the correlations in this model

```{r}
subDataFrame<-turbineTrainData[c('X180','X186','X188','X189','X190','X191','X193','X200')]
corSubData<-cor(subDataFrame)
corSubData


```

Regression Model3

After removing the correlated varibales

```{r}
model3=lm(X190~X180+X186+X189+X191+X200,data=turbineTrainData)
summary(model3)
```

There is a scope to remove the not so important variable X180

Regression Model4

After removing X180
```{r}
model4=lm(X190~X186+X189+X191+X200,data=turbineTrainData)
summary(model4)
trainSSE4=sum(model4$residuals^2)
trainRMSE4=sqrt(trainSSE4/nrow(turbineTrainData))
trainSST=sum((turbineTrainData$X190-mean(turbineTrainData$X190))^2)
trainR2Calc4=1-trainSSE4/trainSST
trainRSqr4=summary(model4)$adj.r.squared
MSE.multipleLinreg=trainSSE4
RMSE.multipleLinReg=trainRMSE4
summary(model4)$sigma

```

So the model4 is the final regression model

__Non Linear models __

__Decision tree__

Creating a Bushy tree
```{r}
library(tree)
tree.train=tree(X190~.,data=turbineTrainData)
plot(tree.train)
text(tree.train,pretty=0)

tree.pred=predict(tree.train)
MSE.bushy=mean((tree.pred-turbineTrainData$X190)^2)
RMSE.bushy=sqrt(MSE.bushy)
RMSE.bushy

```

Determining the prune levels

```{r}
cv.turbine = cv.tree(tree.train)
cv.turbine
plot(cv.turbine$size ,cv.turbine$dev ,type='b', main="Plot between size of the tree and the deviance",xlab = "Size of tree", ylab= "deviance")
```
This gives the tree size for which the deviance will be the smallest. In this case it is at 6 which is also the size of the bushy tree. so no pruning is required

Note that the leaves of the tree are similar to the independent variables in the final regression model 

__Bagging__

```{r}

library(randomForest)
bag.turbine=randomForest(X190 ~.,data=turbineTrainData,mtry=18,importance=TRUE)
bag.predict=predict(bag.turbine)
MSE.bag=mean((bag.predict-turbineTrainData$X190)^2)
RMSE.bag=sqrt(MSE.bag)
RMSE.bag

```

Lets see the importance varaibles 

```{r}

importance(bag.turbine)

```
we find that similar variables as in the case of the regression model have been given the more importance

__Random Forest__
This is a special case of the bagging with the mtry = sqrt of the number of the predictor variables

```{r}
rf.turbine=randomForest(X190 ~.,data=turbineTrainData,mtry=5,importance=TRUE)
rf.turbine
summary(rf.turbine)
rf.predict=predict(rf.turbine)
MSE.rf=mean((rf.predict-turbineTrainData$X190)^2)
RMSE.rf=sqrt(MSE.rf)
RMSE.rf
```

the random forest has imporved a little bit on the bagging model

__Boosting__

```{r}
library(gbm)
boost.tree=gbm(X190~.,data=turbineTrainData,distribution = "gaussian",n.trees = 5000,interaction.depth = 4)
summary(boost.tree)
boost.predictions=predict(boost.tree,n.trees=5000)
MSE.boost=mean((boost.predictions-turbineTrainData$X190)^2)
RMSE.boost=sqrt(MSE.boost)
RMSE.boost
```

We see that the training error is minimum in case of the boosting

Lets see on the test data

Reading the test data set
```{r}
turbineTestData<-read.csv("Wind Turbine Power Data_Test_GE Internal.csv")
summary(turbineTestData)

```

Now applying the various models to on the test data

```{r}
#Regression Model
predMultiReg=predict(model4,newdata=turbineTestData)
MSE.test.multireg=sum((turbineTestData$X190-predMultiReg)^2)
RMSE.test.multireg=sqrt(MSE.test.multireg/nrow(turbineTestData))
RMSE.test.multireg

#Decision Tree
tree.test.pred=predict(tree.train,newdata=turbineTestData)
MSE.test.bushy=mean((tree.test.pred-turbineTestData$X190)^2)
RMSE.test.bushy=sqrt(MSE.test.bushy)
RMSE.test.bushy

#bagging
bag.test.pred=predict(bag.turbine,newdata=turbineTestData)
MSE.test.bag=mean((bag.test.pred-turbineTestData$X190)^2)
RMSE.test.bag=sqrt(MSE.test.bag)
RMSE.test.bag

#Random Forest
rf.test.pred=predict(rf.turbine,newdata=turbineTestData)
MSE.test.rf=mean((rf.test.pred-turbineTestData$X190)^2)
RMSE.test.rf=sqrt(MSE.test.rf)
RMSE.test.rf

#Boosting
boost.test.pred=predict(boost.tree,newdata=turbineTestData,n.trees=5000)
MSE.test.boost=mean((boost.test.pred-turbineTestData$X190)^2)
RMSE.test.boost=sqrt(MSE.test.boost)
RMSE.test.boost

```
We find that the regression model is the best


__Comparing X189 based model with the multivariable model__
```{r}

linreg=lm(X190~X189,data=turbineTrainData)
multipleLinReg=lm(X190~X186+X189+X191+X200,data=turbineTrainData)

```

Plotting the predictions by simple regression compared with that by the multiple variable regression

```{r}
par(xpd=T, mar=par()$mar+c(0,0,0,6))
linreg=lm(X190~X189,data=turbineTrainData)
predMultiReg=predict(model4,newdata=turbineTestData)
predLinReg=predict(linreg,newdata=turbineTestData)
plot(turbineTestData$X190,col="grey",main="Simple Regression vs Multivariable Reg. on test data")
points(predMultiReg,col="red")
points(predLinReg,col="blue")
legend(1570,1570,c("Actual", "Simple Reg","Multivariable Reg"), pch = c(1,1,1), col = c('grey','blue','red'))
par(mar=c(5, 4, 4, 2) + 0.1)

```

comparing the deviations of the residuals by both the models

```{r}
par(xpd=T, mar=par()$mar+c(0,0,0,6))
deviationLinReg=turbineTestData$X190-predLinReg
deviationMultReg=turbineTestData$X190- predMultiReg
plot(deviationLinReg,type="l", col="blue", main="Residuals of Simple Reg vs Multivariable Reg",ylab="Residuals")
points(deviationMultReg,col="red",type="l")
legend(1570,800,c("Actual", "Simple Reg","Multivariable Reg"), pch = c(1,1,1), col = c('grey','blue','red'))
par(mar=c(5, 4, 4, 2) + 0.1)


```