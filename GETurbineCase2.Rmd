---
title: "GETurbine"
output: html_document
---
•  Now use the data from the entire wind farm to perform exploratory analysis and create a predictive model of turbine 190’s power given the power from the rest of the farm.  Does this improve the estimate from above?  


```{r}
filenameToRead<-"train.csv"
turbineTrainData<-read.csv(filenameToRead)
str(turbineTrainData)

```

Performing some explorations

```{r}
#Find the summary of the values
summary(turbineTrainData)

```
summary shows that there are some negative values too ..why are there negative readings?

Now drawing the box plts and the histograms to see the disribution.


```{r}
#boxplots
boxplot(turbineTrainData)

```
The box plot shows that for some of the turbines are producing the data similar to X190 while there are others which are not similar. So they might not have much impact while developing the model
```{r}
#hist

cc<-names(turbineTrainData)
length(cc)
for(j in 2:length(cc)){
  print(j)
  hist(turbineTrainData[,cc[j]],xlab=cc[j])
  j
}

```
So the historams suggest that the distribution is not normal. and there are variations in the distribution


```{r}
#correlations
cor(turbineTrainData)

#the correlation data shows that the data is highly correlated. there are many columns which have a correlation of >0.95
#so we will have to remove the columns to avoid multicolinearity issues

```



```{r}
model1=lm(X190~.,data=turbineTrainData)
summary(model1)
trainSSE1=sum(model1$residuals^2)
trainRMSE1=sqrt(trainSSE1/nrow(turbineTrainData))
trainSST=sum((turbineTrainData$X190-mean(turbineTrainData$X190))^2)
trainR2Calc1=1-trainSSE1/trainSST
trainRSqr1=summary(model1)$adj.r.squared

#the model shows that some of the variables are not significant
#so we remove the coefficients and then try to fit the model again
model2=lm(X190~X180+X186+X188+X189+X191+X193+X200,data=turbineTrainData)
summary(model2)
trainSSE2=sum(model2$residuals^2)
trainRMSE2=sqrt(trainSSE2/nrow(turbineTrainData))
trainR2Calc2=1-trainSSE2/trainSST
trainRSqr2=summary(model2)$adj.r.squared

#get the correlation for the significant columns
subDataFrame<-turbineTrainData[c('X180','X186','X188','X189','X190','X191','X193','X200')]
corSubData<-cor(subDataFrame)
corSubData

```
  
We can see that some of the turbines are quite related to each other 
*186 is quite related to 188,189,190 ..the range of values follows the similar trend this is evident from the box plot also
*188 ans 189 are highly correlated so one of them should be there in the model
*189 is highly correlated to 190 so we need to keeo it. that means we can remove the 188
*191 is highly correlated to 190 so we need to keep it
*193 and 191 are highly correlated. so we can keep one of them . 191 is highy related to 190 so we keep it and remove 193 
*200 is relted to 190, 189,191


So the model after removing the correlted independent variable 
```{r}

model3=lm(X190~X180+X186+X189+X191+X200,data=turbineTrainData)
summary(model3)
trainSSE3=sum(model3$residuals^2)
trainRMSE3=sqrt(trainSSE3/nrow(turbineTrainData))
trainR2Calc3=1-trainSSE3/trainSST
trainRSqr3=summary(model3)$adj.r.squared

# in this model even 180 is not that significant so we can as well remove 180


model4=lm(X190~X186+X189+X191+X200,data=turbineTrainData)
summary(model4)
trainSSE4=sum(model4$residuals^2)
trainRMSE4=sqrt(trainSSE4/nrow(turbineTrainData))
trainR2Calc4=1-trainSSE4/trainSST
trainRSqr4=summary(model4)$adj.r.squared
MSE.linreg=trainSSE4
RMSE.linreg=trainRMSE4
summary(model4)$sigma #same as the RMSE

```
Even though the R2 is going down little bit when we remove the variables but the model is still OK . It is always better to have simple models than the complicated ones

Now finding teh diagnostics
```{r}
par(mfrow=c(2,2))
#diagnostics
plot(model4)

#outliers
library(car)
outliers=outlierTest(model4)
outliers

#get the high lebvergare points
highLevPoints<-influencePlot(model4)
highLevPoints
```

```{r}
par(mfrow=c(1,1))
plot(turbineTrainData$X186)
points(turbineTrainData$X189,col="red")
points(turbineTrainData$X191,col="blue")
points(turbineTrainData$X190,col="green")


predictions=predict(model4)


```


```{r}
library(tree)
par(mfrow=c(2,2))
tree.train=tree(X190~.,data=turbineTrainData)
plot(tree.train)
text(tree.train,pretty=0)

tree.pred=predict(tree.train)
MSE.bushy=mean((tree.pred-turbineTrainData$X190)^2)
RMSE.bushy=sqrt(MSE.bushy)


#now pruning the trees
cv.turbine = cv.tree(tree.train)
cv.turbine
plot(cv.turbine)
plot(cv.turbine$size ,cv.turbine$dev ,type='b')

#so the 6 is the optimal size
prune.turbine = prune.tree(tree.train, best = 6)
plot(prune.turbine)
text(prune.turbine,pretty=0)

prune.pred=predict(prune.turbine)
MSE.prune=mean((prune.pred-turbineTrainData$X190)^2)
RMSE.prune=sqrt(MSE.prune)


```
SO in this case there is no difference in the bushy and the prune trees

```{r}
#lets see what happens if we use the CART
library(rpart)
library(rpart.plot)
tree.CART=rpart(X190~.,data=turbineTrainData)
prp(tree.CART)

CART.pred=predict(tree.CART)

MSE.CART=mean((CART.pred-turbineTrainData$X190)^2)
RMSE.CART=sqrt(MSE.CART)

#this tree seems to be similar to the one drawn using the tree()
```

#Bagging & Random Forest approach

```{r}
library(randomForest)
#trying bagging using the whole set of parameters=11
bag.turbine=randomForest(X190 ~.,data=turbineTrainData,mtry=18,importance=TRUE)
#this plots a curve chowing how the error went down from 1 tree to 25 tree
plot(bag.turbine)
bag.turbine

#as per the model the important variables are
#X186,X187,188,189,191,193,198,200

#NOW THE PREDICTIONS
bag.predict=predict(bag.turbine)
MSE.bag=mean((bag.predict-turbineTrainData$X190)^2)
RMSE.bag=sqrt(MSE.bag)

#this is similar to the model4 linear regression

#RF


oob.err = double(18)
test.err = double(18)
for(i in 1:18){
  fit=randomForest(X190 ~.,data=turbineTrainData,mtry=i)
  oob.err[i]=fit$mse[500]
  pred=predict(fit)
  #test.err[i]=mean((pred-turbineTrainData$X190)^2)
  cat(i," ")
}

oob.err
#test.err

plot(oob.err)

#we can see that oob.err is minimum at 7
rf.turbine=randomForest(X190 ~.,data=turbineTrainData,mtry=7,importance=TRUE)
rf.turbine
summary(rf.turbine)
rf.predict=predict(rf.turbine)
MSE.rf=mean((rf.predict-turbineTrainData$X190)^2)
RMSE.rf=sqrt(MSE.rf)

#matplot(1:i,cbind(test.err,oob.err),pch=19,col=("red","blue"),type="b",ylab="MSE")

```

Boosting

```{r}
library(gbm)
#set.seed(1)
boost.tree=gbm(X190~.,data=turbineTrainData,distribution = "gaussian",n.trees = 5000,interaction.depth = 4)
summary(boost.tree)

boost.predictions=predict(boost.tree,n.trees=5000)
MSE.boost=mean((boost.predictions-turbineTrainData$X190)^2)
RMSE.boost=sqrt(MSE.boost)
```

__summary of all the RMSE__
```{r}
#linerreg
MSE.linreg
RMSE.linreg

#tree
MSE.bushy
RMSE.bushy

#bagging
MSE.bag
RMSE.bag

#Random Forest
MSE.rf
RMSE.rf

#boosting
MSE.boost
RMSE.boost


```

Training error in case of Boosting (RMSE.boost) is the minimum.

```{r}
#read test
turbineTestData<-read.csv("test.csv")
summary(turbineTestData)

```
The test error from the boost

```{r}
#linear regression
predLinReg=predict(model4,newdata=turbineTestData)
MSE.test.linreg=sum((turbineTestData$X190-predLinReg)^2)
RMSE.test.linreg=sqrt(MSE.test.linreg/nrow(turbineTestData))

#tree
tree.test.pred=predict(tree.train,newdata=turbineTestData)
MSE.test.bushy=mean((tree.pred-turbineTestData$X190)^2)
RMSE.test.bushy=sqrt(MSE.test.bushy)

#bagging
bag.test.pred=predict(bag.turbine,newdata=turbineTestData)
MSE.test.bag=mean((bag.test.pred-turbineTestData$X190)^2)
RMSE.test.bag=sqrt(MSE.test.bag)

#Rf
rf.test.pred=predict(rf.turbine,newdata=turbineTestData)
MSE.test.rf=mean((rf.test.pred-turbineTestData$X190)^2)
RMSE.test.rf=sqrt(MSE.test.rf)

#boost
boost.test.pred=predict(boost.tree,newdata=turbineTestData,n.trees=5000)
MSE.test.boost=mean((boost.test.pred-turbineTestData$X190)^2)
RMSE.test.boost=sqrt(MSE.test.boost)


```

We found that even though the training error in case of boosting was lowest , the test error for the linear regression tree is the lowest

```{r}
#can we plot the points and then see which model has predicted better for the majority of the points
linreg1=lm(X190~X189,data=turbineTrainData)
predictionData=predict(linreg1,newdata=turbineTestData,interval="prediction")
predictions<-predictionData[1:nrow(predictionData)]

par(mfrow=c(1,1))

plot(turbineTestData$X190)
points(predictions,col="red")
points(predLinReg,col="blue")

diffLinReg=turbineTestData$X190-predictions
diffMultReg=turbineTestData$X190-predLinReg
plot(diffLinReg,type="l")
points(diffMultReg,col="red",type="l")
boxplot(diffLinReg,diffMultReg,names=c("DiffLinReg","DiffMultReg"))

```
we can clearly see that there are more number of outliers if we try to see the deviation from the actual value in case of the linear reg using only 189
So the Multiregression seems to be a better choice for making the predictions
